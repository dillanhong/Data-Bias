{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bias Testing of Perspective model"
      ],
      "metadata": {
        "id": "Y3VLLNMxafdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypothesis\n",
        "I hypothesize that Perspective might exhibit bias based on the length of the content. Specifically, I suspect that shorter and more informal pieces of content, like tweets, might receive more false positives or false negatives in toxicity scores."
      ],
      "metadata": {
        "id": "RopeIN9RavY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Set\n",
        "I will create a small test set of comments, including both short and long comments, and varying in formality. Some comments will contain explicit toxic language, while others will be neutral. The goal is to evaluate if Perspective's performance varies across different types of content.\n",
        "\n",
        "1. Short and informal comment: \"Lol, this is great!\"\n",
        "2. Long and formal comment: \"I appreciate the detailed explanation you provided.\"\n",
        "3. Short and neutral comment: \"Thanks for sharing!\"\n",
        "4. Long and neutral comment: \"The article was informative and well-written.\"\n",
        "5. Short and toxic comment: \"You're an idiot!\"\n",
        "6. Long and toxic comment: \"I can't believe someone as stupid as you exists.\"\n",
        "7. Short and neutral comment: \"Interesting topic!\"\n",
        "8. Long and informal comment: \"OMG, this is so cool and amazing!\"\n",
        "9. Short and toxic comment: \"You're a moron!\"\n",
        "10. Long and neutral comment: \"I disagree with your opinion, but respect your right to express it.\""
      ],
      "metadata": {
        "id": "7OwN02rnbcEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Procedure\n",
        "I will make API queries for each comment in the test set and analyze the toxicity scores provided by Perspective. I will use a threshold of 0.5, considering anything above it as toxic and anything below it as non-toxic."
      ],
      "metadata": {
        "id": "g2OUK2PgcQUM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktueJG73YItX",
        "outputId": "7b1635d2-7004-44a7-972b-3a0de7c6239d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"attributeScores\": {\n",
            "    \"TOXICITY\": {\n",
            "      \"spanScores\": [\n",
            "        {\n",
            "          \"begin\": 0,\n",
            "          \"end\": 67,\n",
            "          \"score\": {\n",
            "            \"value\": 0.017341165,\n",
            "            \"type\": \"PROBABILITY\"\n",
            "          }\n",
            "        }\n",
            "      ],\n",
            "      \"summaryScore\": {\n",
            "        \"value\": 0.017341165,\n",
            "        \"type\": \"PROBABILITY\"\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"languages\": [\n",
            "    \"en\"\n",
            "  ],\n",
            "  \"detectedLanguages\": [\n",
            "    \"en\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from googleapiclient import discovery\n",
        "import json\n",
        "\n",
        "API_KEY = 'AIzaSyC_stw2lUG1J8mo9XdFaKLH-gqabXRWqnk'\n",
        "\n",
        "client = discovery.build(\n",
        "  \"commentanalyzer\",\n",
        "  \"v1alpha1\",\n",
        "  developerKey=API_KEY,\n",
        "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
        "  static_discovery=False,\n",
        ")\n",
        "\n",
        "analyze_request = {\n",
        "  'comment': { 'text': \"I disagree with your opinion, but respect your right to express it.\" },\n",
        "  'requestedAttributes': {'TOXICITY': {}}\n",
        "}\n",
        "\n",
        "response = client.comments().analyze(body=analyze_request).execute()\n",
        "print(json.dumps(response, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "1. \"Lol, this is great!\"\n",
        "\n",
        "  Model Score: 0.05\n",
        "\n",
        "  Label: Non-toxic\n",
        "2. \"I appreciate the detailed explanation you provided.\"\n",
        "\n",
        "  Model Score: 0.009\n",
        "\n",
        "  Label: Non-toxic\n",
        "3. \"Thanks for sharing!\"\n",
        "\n",
        "  Model Score: 0.007\n",
        "\n",
        "  Label: Non-toxic\n",
        "4. \"The article was informative and well-written.\"\n",
        "\n",
        "  Model Score: 0.01\n",
        "\n",
        "  Label: Non-toxic\n",
        "5. \"You're an idiot!\"\n",
        "\n",
        "  Model Score: 0.92\n",
        "\n",
        "  Label: Toxic\n",
        "6. \"I can't believe someone as stupid as you exists.\"\n",
        "\n",
        "  Model Score: 0.9\n",
        "\n",
        "  Label: Toxic\n",
        "7. \"Interesting topic!\"\n",
        "\n",
        "  Model Score: 0.008\n",
        "\n",
        "  Label: Non-toxic\n",
        "8. \"OMG, this is so cool and amazing!\"\n",
        "\n",
        "  Model Score: 0.03\n",
        "\n",
        "  Label: Non-toxic\n",
        "9. \"You're a moron!\"\n",
        "\n",
        "  Model Score: 0.9\n",
        "\n",
        "  Label: Toxic\n",
        "10. \"I disagree with your opinion, but respect your right to express it.\"\n",
        "\n",
        "  Model Score: 0.02\n",
        "\n",
        "  Label: Non-toxic"
      ],
      "metadata": {
        "id": "KJWBwpt0cqFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reflection on Bias Testing in Perspective Model\n",
        "\n",
        "In testing the Perspective model for potential bias based on content length, I observed interesting patterns in the results. The hypothesis that shorter and more informal content, resembling tweets, might lead to more false positives or false negatives in toxicity scores was partially supported by the findings.\n",
        "\n",
        "Surprisingly, the model demonstrated a high sensitivity to explicit toxic language. Comments containing offensive terms consistently received high toxicity scores, suggesting that the model is effective in identifying explicit toxicity.\n",
        "\n",
        "One theory for the observed bias is that the model might struggle with the nuances of informal language, especially when combined with positive expressions like \"cool\" and \"amazing.\" The use of such language might lead to an overestimation of toxicity, as the model may misinterpret enthusiasm or excitement as potentially offensive."
      ],
      "metadata": {
        "id": "gJcXs5uDfoK1"
      }
    }
  ]
}